{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "        .appName('lakeHouseApp')\n",
    "        .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "    )\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f0e9f78-8d70-4d26-b8bc-eac2a6200a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Expectation(ABC):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        self.column = column\n",
    "        self.dimension = dimension\n",
    "        self.add_info = add_info\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, ge_df):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7ddb51-d2cd-4a34-9d95-c68ed63ea8f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class NotNullExpectation(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_not_be_null(column=self.column,\n",
    "                                                  meta = {\"dimension\": self.dimension},\n",
    "                                                  result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e44c34-3a66-41ed-a871-5af51c1b4d76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UniqueExpectation(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_unique(column=self.column,\n",
    "                                                meta = {\"dimension\": self.dimension}, \n",
    "                                                result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdf084a-fe17-4c7f-96ef-744018ec968c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaf3d551-85f6-4212-bc82-b51883908d07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValuesInListExpectation(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_in_set(column=self.column, \n",
    "                                                value_set=self.add_info[\"value_set\"],\n",
    "                                                meta = {\"dimension\": self.dimension}, \n",
    "                                                result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf8012b-4309-43a5-aee6-1d8dda55cf1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueLengthsToEqual(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_value_lengths_to_equal(column=self.column, \n",
    "                                                   value=self.add_info[\"value\"], \n",
    "                                                   meta = {\"dimension\": self.dimension}, \n",
    "                                                   result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5917320-81b1-4311-9dcb-58917fe66cae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueLengthsToBeBetween(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_value_lengths_to_be_between(column=self.column, \n",
    "                                                        min_value=self.add_info[\"min_value\"], \n",
    "                                                        max_value=self.add_info[\"max_value\"],  \n",
    "                                                        meta = {\"dimension\": self.dimension}, \n",
    "                                                        result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce6c8a8-d556-482f-8b4e-abeee4946974",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeBetween(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_between(column=self.column, \n",
    "                                                 min_value=self.add_info[\"min_value\"], \n",
    "                                                 max_value=self.add_info[\"max_value\"],  \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fa9e81-b2a2-49cf-80f0-b0137a7615c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeGreaterThan(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_between(column=self.column, \n",
    "                                                 min_value=self.add_info[\"min_value\"], \n",
    "                                                 strict_min=True,  \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1597e5-bc26-437e-b15f-fd0f601cba4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeGreaterThanOrEqualTo(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_between(column=self.column, \n",
    "                                                 min_value=self.add_info[\"min_value\"],  \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81b4373-ab2b-4fda-9d55-720a63910b78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeLessThan(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_between(column=self.column, \n",
    "                                                 max_value=self.add_info[\"max_value\"], \n",
    "                                                 strict_max=True, \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f06a0ecd-745e-4509-b806-cb8d961bd39c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeLessThanOrEqualTo(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_between(column=self.column, \n",
    "                                                 max_value=self.add_info[\"max_value\"],  \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5168b23-c909-44cd-a2c2-7bbca77edceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeDateutilParseable(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_dateutil_parseable(column=self.column,  \n",
    "                                                            meta = {\"dimension\": self.dimension}, \n",
    "                                                            result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43642d1d-22d8-4fbf-9af0-3aafaf85d232",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeJsonParseable(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_json_parseable(column=self.column,  \n",
    "                                                        meta = {\"dimension\": self.dimension}, \n",
    "                                                        result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c36e03e4-d406-4970-8edf-b6ab0a011914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValueToBeOfType(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_be_of_type(column=self.column, \n",
    "                                                 type_=self.add_info[\"type\"], \n",
    "                                                 meta = {\"dimension\": self.dimension}, \n",
    "                                                 result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4396fd5-86ff-4695-a314-06e922dd4fb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValuesToMatchRegex(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_match_regex(column=self.column, \n",
    "                                                  regex =self.add_info[\"regex\"], \n",
    "                                                  meta = {\"dimension\": self.dimension}, \n",
    "                                                  result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5782a692-353b-490a-a9cc-52e800c15010",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValuesToNotMatchRegex(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_not_match_regex(column=self.column, \n",
    "                                                      regex =self.add_info[\"regex\"], \n",
    "                                                      meta = {\"dimension\": self.dimension}, \n",
    "                                                      result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bdf64b3-fc4e-478f-88ea-ec92eaa49462",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ValuesToMatchStrftimeFormat(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_values_to_match_strftime_format(column=self.column,\n",
    "                                                            strftime_format=self.add_info[\"format\"],\n",
    "                                                            meta = {\"dimension\": self.dimension},\n",
    "                                                            result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41f21c5-57a2-4959-a015-aaaca1c5e1bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UniqueValuesCountToBeBetween(Expectation):\n",
    "    def __init__(self, column, dimension, add_info = {}):\n",
    "        super().__init__(column, dimension, add_info)\n",
    "\n",
    "    def test(self, ge_df):\n",
    "        ge_df.expect_column_unique_value_count_to_be_between(column=self.column,\n",
    "                                                            min_value=self.add_info[\"min_value\"],\n",
    "                                                            max_value=self.add_info[\"max_value\"], \n",
    "                                                            meta = {\"dimension\": self.dimension},\n",
    "                                                            result_format=\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee29891b-84a6-4c1e-993d-7d3d0301a09a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9d3aa1-c177-40be-802e-872d4bf22a71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JSONFileReader:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def read(self):\n",
    "        with open(self.filename) as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "453e3611-754c-4ef1-bf6c-5e5236efd5e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset\n",
    "\n",
    "class DataQuality:\n",
    "\n",
    "    def __init__(self, pyspark_df, config_path):\n",
    "        self.pyspark_df = pyspark_df\n",
    "        self.config_path = config_path\n",
    "\n",
    "    def rule_mapping(self, dq_rule):\n",
    "        return {\n",
    "            \"check_values_to_not_be_null\": \"NotNullExpectation\",\n",
    "            \"check_values_to_be_unique\": \"UniqueExpectation\",\n",
    "            \"check_values_to_be_in_set\": \"ValuesInListExpectation\",\n",
    "            \"check_value_lengths_to_equal\": \"ValueLengthsToEqual\",\n",
    "            \"check_value_lengths_to_be_between\": \"ValueLengthsToBeBetween\",\n",
    "            \"check_values_to_be_between\": \"ValueToBeBetween\",\n",
    "            \"check_values_to_be_greater_than\": \"ValueToBeGreaterThan\",\n",
    "            \"check_values_to_be_greater_than_or_equal_to\": \"ValueToBeGreaterThanOrEqualTo\",\n",
    "            \"check_values_to_be_less_than\": \"ValueToBeLessThan\",\n",
    "            \"check_values_to_be_less_than_or_equal_to\": \"ValueToBeLessThanOrEqualTo\",\n",
    "            \"check_values_to_be_dateutil_parseable\": \"ValueToBeDateutilParseable\",\n",
    "            \"check_values_to_be_json_parseable\": \"ValueToBeJsonParseable\",\n",
    "            \"check_values_to_be_of_type\": \"ValueToBeOfType\",\n",
    "            \"check_values_to_match_regex\": \"ValuesToMatchRegex\",\n",
    "            \"check_values_to_not_match_regex\": \"ValuesToNotMatchRegex\",\n",
    "            \"check_values_to_match_strftime_format\": \"ValuesToMatchStrftimeFormat\",\n",
    "            \"check_unique_value_count_to_be_between\": \"UniqueValuesCountToBeBetween\"\n",
    "        }[dq_rule]\n",
    "\n",
    "    def _get_expectation(self):\n",
    "        class_obj = globals()[self.rule_mapping()]\n",
    "        return class_obj(self.extractor_args)\n",
    "    \n",
    "    def convert_to_ge_df(self):\n",
    "        return SparkDFDataset(self.pyspark_df)\n",
    "    \n",
    "    def read_config(self):\n",
    "        json_reader = JSONFileReader(self.config_path)\n",
    "        return json_reader.read()\n",
    "      \n",
    "    def run_test(self):\n",
    "        ge_df = self.convert_to_ge_df()\n",
    "        config = self.read_config()\n",
    "        # config = json.load(conf)\n",
    "        print(config)\n",
    "        for column in config[\"columns\"]:\n",
    "            if column[\"dq_rule(s)\"] is None:\n",
    "                continue\n",
    "            for dq_rule in column[\"dq_rule(s)\"]:\n",
    "                expectation_obj = globals()[self.rule_mapping(dq_rule[\"rule_name\"])]\n",
    "                expectation_instance = expectation_obj(column[\"column_name\"], dq_rule[\"rule_dimension\"], dq_rule[\"add_info\"])\n",
    "                expectation_instance.test(ge_df)\n",
    "\n",
    "        dq_results = ge_df.validate()\n",
    "        return dq_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d33d7d-bc97-4e60-9426-6f04bbc0d8b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_df_from_dq_results(spark, dq_results):\n",
    "    dq_data = []\n",
    "    # print(dq_results)\n",
    "    for result in dq_results[\"results\"]:\n",
    "        if result[\"success\"] == True:\n",
    "            status = 'PASSED'\n",
    "        else:\n",
    "            status = 'FAILED'\n",
    "        unexpected_list = result[\"result\"][\"unexpected_list\"]\n",
    "        # print(len(unexpected_list))\n",
    "        dq_data.append((\n",
    "        result[\"expectation_config\"][\"kwargs\"][\"column\"],\n",
    "        result[\"expectation_config\"][\"meta\"][\"dimension\"],\n",
    "        status,\n",
    "        result[\"expectation_config\"][\"expectation_type\"],\n",
    "        result[\"result\"][\"unexpected_count\"],\n",
    "        result[\"result\"][\"element_count\"],\n",
    "        result[\"result\"][\"unexpected_percent\"],\n",
    "        float(100-result[\"result\"][\"unexpected_percent\"]),\n",
    "        result[\"result\"][\"unexpected_list\"])\n",
    "        )\n",
    "    dq_columns = [\"column\", \"dimension\", \"status\", \"expectation_type\", \"unexpected_count\", \"element_count\", \"unexpected_percent\", \"percent\",\"unexpected_values\"]\n",
    "    dq_df = spark.createDataFrame(data=dq_data,schema=dq_columns)\n",
    "    return dq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23dd34ec-d943-4a73-8f5a-793c37ab2bab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dqcheck(dqdf , config_path):\n",
    "    dq = DataQuality(dqdf, config_path)\n",
    "    dq_results = dq.run_test()\n",
    "    dq_df = create_df_from_dq_results(spark, dq_results)\n",
    "    return(dq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.parquet(\"landing/mongodb/WideWorldImporters/Colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 22:47:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_product_name': 'Colors', 'columns': [{'column_name': 'ColorName', 'dq_rule(s)': [{'rule_name': 'check_values_to_not_be_null', 'rule_dimension': 'Completeness', 'add_info': {}}, {'rule_name': 'check_values_to_be_unique', 'rule_dimension': 'Uniqueness', 'add_info': {}}, {'rule_name': 'check_values_to_be_in_set', 'rule_dimension': 'Validity', 'add_info': {'value_set': ['Red', 'Green', 'Yellow', 'Hindi', 'Sanskrit']}}]}, {'column_name': 'ColorID', 'dq_rule(s)': [{'rule_name': 'check_values_to_not_be_null', 'rule_dimension': 'Completeness', 'add_info': {}}, {'rule_name': 'check_values_to_be_unique', 'rule_dimension': 'Uniqueness', 'add_info': {}}, {'rule_name': 'check_values_to_be_in_set', 'rule_dimension': 'Validity', 'add_info': {'value_set': [1, 2, 3, 4, 5, 6]}}]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 22:47:29 WARN CacheManager: Asked to cache already cached data.\n",
      "24/04/04 22:47:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "dq_df=dqcheck(df,\"dqconfig/Colors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+-----------------------------------+----------------+-------------+------------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|column   |dimension   |status|expectation_type                   |unexpected_count|element_count|unexpected_percent|percent           |unexpected_values                                                                                                                                                                                                                                                                                       |\n",
      "+---------+------------+------+-----------------------------------+----------------+-------------+------------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ColorName|Completeness|PASSED|expect_column_values_to_not_be_null|0               |36           |0.0               |100.0             |[]                                                                                                                                                                                                                                                                                                      |\n",
      "|ColorName|Uniqueness  |PASSED|expect_column_values_to_be_unique  |0               |36           |0.0               |100.0             |[]                                                                                                                                                                                                                                                                                                      |\n",
      "|ColorName|Validity    |FAILED|expect_column_values_to_be_in_set  |34              |36           |94.44444444444444 |5.555555555555557 |[Light Green, Orange, Olive, Ivory, Chartreuse, BlaPitchBlack, Dark Green, Navy Blue, Cyan, Royal Blue, Teal, Steel Gray, Charcoal, Plum, Hot Pink, Maroon, Indigo, Dark Brown, Blue, Beige, Wheat, Azure, Fuchsia, Tan, Silver, Khaki, Purple, Puce, Light Brown, White, Mauve, Salmon, Lavender, Gold]|\n",
      "|ColorID  |Completeness|PASSED|expect_column_values_to_not_be_null|0               |36           |0.0               |100.0             |[]                                                                                                                                                                                                                                                                                                      |\n",
      "|ColorID  |Uniqueness  |PASSED|expect_column_values_to_be_unique  |0               |36           |0.0               |100.0             |[]                                                                                                                                                                                                                                                                                                      |\n",
      "|ColorID  |Validity    |FAILED|expect_column_values_to_be_in_set  |30              |36           |83.33333333333334 |16.666666666666657|[28, 19, 24, 23, 15, 9, 22, 7, 29, 33, 12, 25, 13, 20, 14, 8, 34, 36, 10, 32, 31, 16, 27, 26, 18, 35, 21, 30, 17, 11]                                                                                                                                                                                   |\n",
      "+---------+------------+------+-----------------------------------+----------------+-------------+------------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dq_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_df.createOrReplaceTempView(\"dqtbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def sparkSchemaBuild(spark,dataDict_config_path,data_dict_mapping_config_path,tableNm):\n",
    "\n",
    "    dataDictDF = spark.read.option(\"multiline\",\"true\").json(path=dataDict_config_path)\n",
    "    pdf = pd.DataFrame()\n",
    "    f = open(data_dict_mapping_config_path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    sourceColType = []\n",
    "    targetColType = []\n",
    "    for (k, v) in data.items():\n",
    "        sourceColType.append(k)\n",
    "        targetColType.append(str(v))\n",
    "\n",
    "    data = {'sourceColType': sourceColType,\n",
    "    'targetColType': targetColType}\n",
    "# Convert the dictionary into DataFrame\n",
    "    pdf = pd.DataFrame(data)\n",
    "    datatypeDF=spark.createDataFrame(pdf) \n",
    "    dataDictDF.createOrReplaceTempView(\"dataDict\")\n",
    "    datatypeDF.createOrReplaceTempView(\"dataType\")\n",
    "    schemaDF = spark.sql(\"select dataDict.ColumnName || ' ' || dataType.targetColType  ddl_schema_string ,  CAST(ColumnOrdinal AS INT) ORDINAL_POSITION from dataDict JOIN dataType ON dataDict.ColumnDataType = dataType.sourceColType where  dataDict.TableName = '\" +tableNm+\"'\")\n",
    "    schemaDF =schemaDF.sort(col(\"ORDINAL_POSITION\"))\n",
    "    ddl_schema = schemaDF.collect()\n",
    "    ddl_schema_string = ''\n",
    "    for x in ddl_schema:\n",
    "        ddl_schema_string = ddl_schema_string + ',' + x[0]\n",
    "    ddl_schema_string = ddl_schema_string.lstrip(',')\n",
    "    return ddl_schema_string\n",
    "    # ddl_schema = T._parse_datatype_string(ddl_schema_string.lstrip(','))\n",
    "    # return ddl_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorID int,ColorName string,LastEditedBy int,ValidFrom timestamp,ValidTo timestamp\n"
     ]
    }
   ],
   "source": [
    "dataDictConfigPath = \"/home/animesh/pysparketlframework/lakeHouseBuildFrameWork/etl-config/datadict.json\"\n",
    "dataDictMappingConfigPath = \"/home/animesh/pysparketlframework/lakeHouseBuildFrameWork/etl-config/mongoToSparkMapping.json\"\n",
    "tableNm=\"Colors\"\n",
    "ddl_schema = sparkSchemaBuild(spark,dataDictConfigPath,dataDictMappingConfigPath,tableNm)\n",
    "print(ddl_schema)\n",
    "colList = ddl_schema.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorName\n",
      "string\n",
      "'Light Green','Orange','Olive','Ivory','Chartreuse','BlaPitchBlack','Dark Green','Navy Blue','Cyan','Royal Blue','Teal','Steel Gray','Charcoal','Plum','Hot Pink','Maroon','Indigo','Dark Brown','Blue','Beige','Wheat','Azure','Fuchsia','Tan','Silver','Khaki','Purple','Puce','Light Brown','White','Mauve','Salmon','Lavender','Gold'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorID\n",
      "int\n",
      "28,19,24,23,15,9,22,7,29,33,12,25,13,20,14,8,34,36,10,32,31,16,27,26,18,35,21,30,17,11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.createOrReplaceTempView(\"srctbl\")\n",
    "failedDF = spark.sql(\"select * from dqtbl where status = 'FAILED'\")\n",
    "rows_looped = failedDF.select(\"column\", \"unexpected_values\",\"dimension\").collect()\n",
    "\n",
    "for rows in rows_looped:\n",
    "       \n",
    "        print(rows[0])\n",
    "        for col in colList:\n",
    "            if col.split(\" \")[0] == rows[0]:\n",
    "                dtype = col.split(\" \")[1]\n",
    "                break\n",
    "        print(dtype)    \n",
    "        listofvals = \"\"\n",
    "        for val in rows[1]:\n",
    "            if dtype == \"string\":\n",
    "                listofvals = listofvals +\"'\"+ val + \"',\"\n",
    "            elif dtype == \"int\":  \n",
    "                listofvals = listofvals + val + \",\"\n",
    "        print(listofvals.rstrip(\",\"))\n",
    "        errdf = spark.sql(\"select * from srctbl where \"+rows[0]+\" in (\"+listofvals.rstrip(\",\")+\")\")\n",
    "        errPath = \"/home/animesh/pysparketlframework/lakeHouseBuildFrameWork/err/\"+tableNm+\"/\"\n",
    "        errdf=errdf.withColumn(\"error_desc\",F.lit(rows[2]))\n",
    "        errdf.write.mode(\"append\").format(\"parquet\").partitionBy(\"part_date\").save(errPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "+-------+-------------+------------+---------+-------+--------------------+----------+----------+\n",
      "|ColorID|    ColorName|LastEditedBy|ValidFrom|ValidTo|     IngestTimeStamp|error_desc| part_date|\n",
      "+-------+-------------+------------+---------+-------+--------------------+----------+----------+\n",
      "|     19|  Light Green|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     24|       Orange|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     23|        Olive|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     15|        Ivory|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      6|   Chartreuse|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      3|BlaPitchBlack|           2|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      9|   Dark Green|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     22|    Navy Blue|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      7|         Cyan|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     29|   Royal Blue|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     33|         Teal|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     12|   Steel Gray|           9|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      5|     Charcoal|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     25|         Plum|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     13|     Hot Pink|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     20|       Maroon|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     14|       Indigo|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|      7|         Cyan|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     29|   Royal Blue|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "|     33|         Teal|           1|      NaN|    NaN|2024-03-13 23:21:...|  Validity|2024-03-13|\n",
      "+-------+-------------+------------+---------+-------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baddf = spark.read.parquet(errPath)\n",
    "print(baddf.count())\n",
    "badf =baddf.drop(\"error_desc\")\n",
    "# Perform the exceptAll() operation\n",
    "goodDF = df.exceptAll(baddf)\n",
    "# Display the resulting DataFrame\n",
    "goodDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+---------+-------+-----------------------+----------+\n",
      "|ColorID|ColorName    |LastEditedBy|ValidFrom|ValidTo|IngestTimeStamp        |part_date |\n",
      "+-------+-------------+------------+---------+-------+-----------------------+----------+\n",
      "|28     |Red          |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|19     |Light Green  |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|24     |Orange       |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|23     |Olive        |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|15     |Ivory        |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|6      |Chartreuse   |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|3      |BlaPitchBlack|2           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|9      |Dark Green   |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|22     |Navy Blue    |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|7      |Cyan         |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|29     |Royal Blue   |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|33     |Teal         |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|12     |Steel Gray   |9           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|5      |Charcoal     |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|25     |Plum         |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|13     |Hot Pink     |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|20     |Maroon       |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|14     |Indigo       |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|8      |Dark Brown   |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "|4      |Blue         |1           |NaN      |NaN    |2024-03-13 23:21:37.292|2024-03-13|\n",
      "+-------+-------------+------------+---------+-------+-----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dq_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "GE DQ Rules Test",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
